{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inesgoddi/textLevelClassifier/blob/main/Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94afa228",
      "metadata": {
        "id": "94afa228"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from transformers import BertForSequenceClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install CUDA Toolkit\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-11-0_11.0.3-1_amd64.deb\n",
        "!dpkg -i cuda-11-0_11.0.3-1_amd64.deb\n",
        "!apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda\n",
        "\n",
        "# Install cuDNN (adjust version accordingly)\n",
        "!wget https://developer.download.nvidia.com/compute/redist/cudnn/v8.0.5/cudnn-11.0-linux-x64-v8.0.5.39.tgz\n",
        "!tar -xzvf cudnn-11.0-linux-x64-v8.0.5.39.tgz\n",
        "!cp cuda/include/cudnn*.h /usr/local/cuda/include\n",
        "!cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\n",
        "!chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwSfkiAtjl0o",
        "outputId": "b817d306-b846-453b-87be-a635be4d963f"
      },
      "id": "VwSfkiAtjl0o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-08 00:18:01--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-11-0_11.0.3-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.20.126\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.20.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2446 (2.4K) [application/x-deb]\n",
            "Saving to: ‘cuda-11-0_11.0.3-1_amd64.deb’\n",
            "\n",
            "cuda-11-0_11.0.3-1_ 100%[===================>]   2.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-08 00:18:01 (208 MB/s) - ‘cuda-11-0_11.0.3-1_amd64.deb’ saved [2446/2446]\n",
            "\n",
            "Selecting previously unselected package cuda-11-0.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack cuda-11-0_11.0.3-1_amd64.deb ...\n",
            "Unpacking cuda-11-0 (11.0.3-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of cuda-11-0:\n",
            " cuda-11-0 depends on cuda-runtime-11-0 (>= 11.0.3); however:\n",
            "  Package cuda-runtime-11-0 is not installed.\n",
            " cuda-11-0 depends on cuda-toolkit-11-0 (>= 11.0.3); however:\n",
            "  Package cuda-toolkit-11-0 is not installed.\n",
            " cuda-11-0 depends on cuda-demo-suite-11-0 (>= 11.0.167); however:\n",
            "  Package cuda-demo-suite-11-0 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package cuda-11-0 (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Errors were encountered while processing:\n",
            " cuda-11-0\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "Executing: /tmp/apt-key-gpghome.HFmhAaMyYG/gpg.1.sh --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
            "gpg: requesting key from 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub'\n",
            "gpg: key F60F4B3D7FA2AF80: public key \"cudatools <cudatools@nvidia.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,125 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,994 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,591 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,664 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [973 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,258 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,409 kB]\n",
            "Fetched 13.4 MB in 1s (12.6 MB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "You might want to run 'apt --fix-broken install' to correct these.\n",
            "The following packages have unmet dependencies:\n",
            " cuda : Depends: cuda-12-5 (>= 12.5.1) but it is not going to be installed\n",
            " cuda-11-0 : Depends: cuda-runtime-11-0 (>= 11.0.3) but it is not installable\n",
            "             Depends: cuda-toolkit-11-0 (>= 11.0.3) but it is not installable\n",
            "             Depends: cuda-demo-suite-11-0 (>= 11.0.167) but it is not installable\n",
            "E: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution).\n",
            "--2024-07-08 00:18:07--  https://developer.download.nvidia.com/compute/redist/cudnn/v8.0.5/cudnn-11.0-linux-x64-v8.0.5.39.tgz\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.20.126\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.20.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1085996495 (1.0G) [application/x-compressed]\n",
            "Saving to: ‘cudnn-11.0-linux-x64-v8.0.5.39.tgz’\n",
            "\n",
            "cudnn-11.0-linux-x6 100%[===================>]   1.01G  25.3MB/s    in 37s     \n",
            "\n",
            "2024-07-08 00:18:44 (27.8 MB/s) - ‘cudnn-11.0-linux-x64-v8.0.5.39.tgz’ saved [1085996495/1085996495]\n",
            "\n",
            "cuda/include/cudnn.h\n",
            "cuda/include/cudnn_adv_infer.h\n",
            "cuda/include/cudnn_adv_train.h\n",
            "cuda/include/cudnn_backend.h\n",
            "cuda/include/cudnn_cnn_infer.h\n",
            "cuda/include/cudnn_cnn_train.h\n",
            "cuda/include/cudnn_ops_infer.h\n",
            "cuda/include/cudnn_ops_train.h\n",
            "cuda/include/cudnn_version.h\n",
            "cuda/NVIDIA_SLA_cuDNN_Support.txt\n",
            "cuda/lib64/libcudnn.so\n",
            "cuda/lib64/libcudnn.so.8\n",
            "cuda/lib64/libcudnn.so.8.0.5\n",
            "cuda/lib64/libcudnn_adv_infer.so\n",
            "cuda/lib64/libcudnn_adv_infer.so.8\n",
            "cuda/lib64/libcudnn_adv_infer.so.8.0.5\n",
            "cuda/lib64/libcudnn_adv_train.so\n",
            "cuda/lib64/libcudnn_adv_train.so.8\n",
            "cuda/lib64/libcudnn_adv_train.so.8.0.5\n",
            "cuda/lib64/libcudnn_cnn_infer.so\n",
            "cuda/lib64/libcudnn_cnn_infer.so.8\n",
            "cuda/lib64/libcudnn_cnn_infer.so.8.0.5\n",
            "cuda/lib64/libcudnn_cnn_train.so\n",
            "cuda/lib64/libcudnn_cnn_train.so.8\n",
            "cuda/lib64/libcudnn_cnn_train.so.8.0.5\n",
            "cuda/lib64/libcudnn_ops_infer.so\n",
            "cuda/lib64/libcudnn_ops_infer.so.8\n",
            "cuda/lib64/libcudnn_ops_infer.so.8.0.5\n",
            "cuda/lib64/libcudnn_ops_train.so\n",
            "cuda/lib64/libcudnn_ops_train.so.8\n",
            "cuda/lib64/libcudnn_ops_train.so.8.0.5\n",
            "cuda/lib64/libcudnn_static.a\n",
            "cuda/lib64/libcudnn_static.a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orBe601mjJnc",
        "outputId": "36fd60c1-8891-419b-82ac-f86e0e298ead"
      },
      "id": "orBe601mjJnc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul  7 18:11:12 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJEheAnZi9us",
        "outputId": "a69540dc-4e66-48ad-e60a-173c6b57e9ae"
      },
      "id": "zJEheAnZi9us",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deG-OBYYBGDH",
        "outputId": "d9c14783-a963-4be8-920f-16ac270b1548"
      },
      "id": "deG-OBYYBGDH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet"
      ],
      "metadata": {
        "id": "ZCLb0X_x-qx_"
      },
      "id": "ZCLb0X_x-qx_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the first Excel file into a pandas DataFrame\n",
        "file1 = \"debutants.xlsx\"\n",
        "df1 = pd.read_excel(file1)\n",
        "\n",
        "# Read the second Excel file into a pandas DataFrame\n",
        "file2 = \"expert.xlsx\"\n",
        "df2 = pd.read_excel(file2)\n",
        "\n",
        "# Merge or concatenate the DataFrames\n",
        "# For example, if you want to concatenate them row-wise:\n",
        "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
        "# Assuming your data is stored in X (features) and y (labels) lists or arrays\n",
        "X_train, X_val, y_train, y_val = train_test_split(merged_df.iloc[:, 0], merged_df.iloc[:, 1], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "X7LjkoYOIQq7"
      },
      "id": "X7LjkoYOIQq7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define column names\n",
        "column_names = ['text', 'label']  # Add column names as needed\n",
        "\n",
        "# Assign column names directly to the merged DataFrame\n",
        "merged_df.columns = column_names\n",
        "\n",
        "# Print the merged DataFrame to verify the header\n",
        "print(len(merged_df.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q7fteicKOZ7",
        "outputId": "ff0553ce-46d4-42e7-9020-bf69544cd5db"
      },
      "id": "3Q7fteicKOZ7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datapath = f'test.csv'\n",
        "#df = pd.read_csv(datapath,index_col=False)\n",
        "#df.reset_index(drop=True, inplace=True)\n",
        "with open('test.csv', 'rb') as f:\n",
        "    encoding_test = chardet.detect(f.read())['encoding']\n",
        "test_df = pd.read_csv(test_datapath, encoding=encoding_test)\n"
      ],
      "metadata": {
        "id": "l7lWy5NTgWRp"
      },
      "id": "l7lWy5NTgWRp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ef3518",
      "metadata": {
        "id": "92ef3518"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', output_attentions=True, attn_implementation=\"eager\")\n",
        "\n",
        "labels = {\n",
        "    '0': 0,\n",
        "    '1': 1\n",
        "}\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, max_length=512, chunking_strategy=\"longest_first\"):\n",
        "        self.max_length = max_length\n",
        "        self.chunking_strategy = chunking_strategy\n",
        "\n",
        "        self.labels = [labels[label] for label in df['label'].astype(str)]\n",
        "        self.texts = []\n",
        "\n",
        "        for text in df['text']:\n",
        "            # Tokenize and chunk the text into manageable parts\n",
        "            tokenized_text = tokenizer.encode_plus(\n",
        "                text,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                return_overflowing_tokens=True,\n",
        "                stride=self.max_length - 128  # Adjust the stride as needed\n",
        "            )\n",
        "\n",
        "            # Extract individual chunks from tokenized_text\n",
        "            for i in range(len(tokenized_text['input_ids'])):\n",
        "                chunk = {}\n",
        "                for key in tokenized_text.keys():\n",
        "                    chunk[key] = tokenized_text[key][i]\n",
        "                self.texts.append(chunk)\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased', output_attentions=True, attn_implementation=\"eager\")\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "        self.cls_embedding = None\n",
        "        #self.attentions = None\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        self.cls_embedding = outputs.last_hidden_state[:, 0, :]  # Extract CLS token embedding (first token)\n",
        "        #self.attentions = outputs.attentions  # Store attentions for visualization\n",
        "\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = self.fc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "iZbkbBiTHW9J"
      },
      "id": "iZbkbBiTHW9J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=1)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "    cls_embeddings_with_text = []\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "            num_batches = len(train_dataloader)\n",
        "\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].squeeze(1).to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                cls_embedding = model.cls_embedding\n",
        "                #attentions = model.attentions\n",
        "\n",
        "                labels = [label for label in train_label]\n",
        "                decoded_texts = [tokenizer.decode(ids,skip_special_tokens=True) for ids in input_id]\n",
        "\n",
        "                for text,embedding,label in zip(decoded_texts,cls_embedding,labels):\n",
        "                  cls_embeddings_with_text.append((text,embedding,label))\n",
        "\n",
        "                batch_loss = criterion(output, train_label.long())\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].squeeze(1).to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label.long())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "\n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "    return cls_embeddings_with_text"
      ],
      "metadata": {
        "id": "uoXazpaVXNjr"
      },
      "id": "uoXazpaVXNjr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    cls_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              mask = test_input['attention_mask'].squeeze(1).to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "              cls_embedding = model.cls_embedding\n",
        "              #attentions = model.attentions\n",
        "\n",
        "\n",
        "              labels = [label for label in test_label]\n",
        "              decoded_texts = [tokenizer.decode(ids,skip_special_tokens=True) for ids in input_id]\n",
        "\n",
        "              for text,embedding,label in zip(decoded_texts,output,labels):\n",
        "                cls_embeddings.append((text,cls_embedding,label))\n",
        "\n",
        "\n",
        "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "              total_acc_test += acc\n",
        "\n",
        "\n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "    return cls_embeddings"
      ],
      "metadata": {
        "id": "HtwwVfN9XOnd"
      },
      "id": "HtwwVfN9XOnd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(112)\n",
        "train_df, val_df, test_df_2 = np.split(merged_df.sample(frac=1, random_state=42),\n",
        "                                     [int(.8*len(merged_df)), int(.9*len(merged_df))])\n",
        "\n",
        "print(len(train_df),len(val_df), len(test_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUr_vDLZXUrs",
        "outputId": "ef9d7b37-e4b1-4864-8fcc-703a54f825a0"
      },
      "id": "MUr_vDLZXUrs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95 12 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "num_classes = 2\n",
        "model = BertClassifier()\n",
        "LR = 2e-5\n",
        "\n",
        "cls_embeddings_with_text = train(model, train_df, val_df, LR, EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swf9sIh3XXPP",
        "outputId": "5cf487dc-c11a-4003-acba-20fba0f7089f"
      },
      "id": "Swf9sIh3XXPP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.162 | Train Accuracy:  0.947 | Val Loss:  0.024 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.015 | Train Accuracy:  1.000 | Val Loss:  0.008 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.005 | Train Accuracy:  1.000 | Val Loss:  0.004 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss:  0.003 | Train Accuracy:  1.000 | Val Loss:  0.002 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss:  0.002 | Train Accuracy:  1.000 | Val Loss:  0.002 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 6 | Train Loss:  0.001 | Train Accuracy:  1.000 | Val Loss:  0.001 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 7 | Train Loss:  0.001 | Train Accuracy:  1.000 | Val Loss:  0.001 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 8 | Train Loss:  0.001 | Train Accuracy:  1.000 | Val Loss:  0.001 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 9 | Train Loss:  0.001 | Train Accuracy:  1.000 | Val Loss:  0.001 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 10 | Train Loss:  0.001 | Train Accuracy:  1.000 | Val Loss:  0.001 | Val Accuracy:  1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_embeddings = evaluate(model, test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgeInPCDkLKo",
        "outputId": "7f4f284d-bf29-41fc-ad46-6e4e8f2fcf79"
      },
      "id": "MgeInPCDkLKo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy:  0.889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.random.seed(112)\n",
        "train_df, val_df, test_df_2 = np.split(merged_df.sample(frac=1, random_state=42),\n",
        "                                     [int(.8*len(merged_df)), int(.9*len(merged_df))])\n",
        "\n",
        "EPOCHS = 10\n",
        "num_classes = 2\n",
        "test_model = BertClassifier()\n",
        "LR = 2e-5\n",
        "\n",
        "cls_embeddings_with_text = train(test_model, train_df, val_df, LR, EPOCHS)\n",
        "cls_embeddings = evaluate(test_model, test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRE5yOiC3hCT",
        "outputId": "d107bba6-c21c-4695-e508-f0e009d98e1f"
      },
      "id": "WRE5yOiC3hCT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.240 | Train Accuracy:  0.926 | Val Loss:  0.045 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.025 | Train Accuracy:  1.000 | Val Loss:  0.012 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.008 | Train Accuracy:  1.000 | Val Loss:  0.006 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss:  0.004 | Train Accuracy:  1.000 | Val Loss:  0.003 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss:  0.003 | Train Accuracy:  1.000 | Val Loss:  0.002 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 6 | Train Loss:  0.002 | Train Accuracy:  1.000 | Val Loss:  0.002 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 7 | Train Loss:  0.001 | Train Accuracy:  1.000 | Val Loss:  0.001 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 8 | Train Loss:  0.001 | Train Accuracy:  1.000 | Val Loss:  0.001 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 9 | Train Loss:  0.001 | Train Accuracy:  1.000 | Val Loss:  0.001 | Val Accuracy:  1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [00:12<00:00,  7.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 10 | Train Loss:  0.001 | Train Accuracy:  1.000 | Val Loss:  0.001 | Val Accuracy:  1.000\n",
            "Test Accuracy:  0.944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def top_k_indices(similarity_matrix, k):\n",
        "    np.fill_diagonal(similarity_matrix, -np.inf)  # Exclude self-similarity\n",
        "    flattened_indices = np.argsort(similarity_matrix.flatten())[::-1][:k]\n",
        "    row_indices, col_indices = np.unravel_index(flattened_indices, similarity_matrix.shape)\n",
        "    return row_indices, col_indices\n",
        "\n",
        "def find_most_similar_tensors(tensors_list, k=10):\n",
        "    \"\"\"\n",
        "    Find the k most similar tensors from a list of tensors using cosine similarity.\n",
        "\n",
        "    Args:\n",
        "    - tensors_list (list of torch.Tensor): A list containing tensors.\n",
        "    - k (int): The number of most similar tensors to return.\n",
        "\n",
        "    Returns:\n",
        "    - most_similar_tensors (list of torch.Tensor): The k most similar tensors.\n",
        "    \"\"\"\n",
        "    # Compute pairwise cosine similarities between tensors\n",
        "\n",
        "    similarities = np.zeros((len(tensors_list), len(tensors_list)))\n",
        "    for i, tensor1 in enumerate(tensors_list):\n",
        "        for j, tensor2 in enumerate(tensors_list):\n",
        "            cls_vector1_normalized = tensor1.cpu().detach().numpy() / np.linalg.norm(tensor1.cpu().detach().numpy())\n",
        "            cls_vector2_normalized = tensor2.cpu().detach().numpy() / np.linalg.norm(tensor2.cpu().detach().numpy())\n",
        "            similarity = cosine_similarity([cls_vector1_normalized], [cls_vector2_normalized])[0][0]\n",
        "            similarities[i][j] = similarity\n",
        "    row_indices, col_indices = top_k_indices(similarities, k)\n",
        "    most_similar_tensors = []\n",
        "    print(\"\\nTop\", k, \"most similar tensors (excluding self-similarity):\")\n",
        "    for i in range(k):\n",
        "      row_index = row_indices[i]\n",
        "      col_index = col_indices[i]\n",
        "      similarity_score = similarities[row_index, col_index]\n",
        "      tensor = tensors_list[row_index]  # Get the tensor from the tensors_list\n",
        "      print(\"Tensor Index:\", row_index)\n",
        "      print(\"Similarity Score:\", similarity_score)\n",
        "      #print(\"Tensor:\", tensor)\n",
        "      most_similar_tensors.append(tensor)\n",
        "    return most_similar_tensors\n",
        "\n",
        "# Example usage:\n",
        "# Assuming tensors_list is a list containing 90 tensors\n",
        "tensors_list_beginner =  [t[1] for t in cls_embeddings_with_text if t[2].item() == 0]\n",
        "most_similar_tensors_beginner = find_most_similar_tensors(tensors_list_beginner, k=10) ## ici verifier pourquoi quand k = 2 je retourne 2 embeddings au lieu de 2 paires de embeddings - c'est possible qu'il y ai des regroupements dans les paires\n",
        "# les paires les plus similaires ne sont pas representatives de donnees hors corpus\n",
        "# novices et experts confondus -considerer la classe majoritaire- nombre expert / nombre total (0 etant le plus novice possible)- et proceder par majorite ou le ratio pour classifier- obtenir un embedding pour representer chaque classe, ce n'est possible avec les paires\n",
        "#ou bien TODO : faire la moyenne de l'ensemble :\n",
        "#mean_tensor_beginner = torch.mean(torch.stack(most_similar_tensors_beginner), dim=0)\n",
        "#mean_tensor_expert = torch.mean(torch.stack(most_similar_tensors_expert), dim=0)\n",
        "#integrer nouvelles donnnesm\n",
        "#TODO : trouver un embedding de reference pour chaque classe\n",
        "\n",
        "\n",
        "# Print the most similar tensors\n",
        "\n",
        "tensors_list_expert =  [t[1] for t in cls_embeddings_with_text if t[2].item() == 1]\n",
        "most_similar_tensors_expert = find_most_similar_tensors(tensors_list_expert, k=10)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbUv9vGLwDJR",
        "outputId": "a896a993-fa42-4634-899d-fa5d9fca92de"
      },
      "id": "TbUv9vGLwDJR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 most similar tensors (excluding self-similarity):\n",
            "Tensor Index: 249\n",
            "Similarity Score: 0.9999986886978149\n",
            "Tensor Index: 250\n",
            "Similarity Score: 0.9999986886978149\n",
            "Tensor Index: 458\n",
            "Similarity Score: 0.9999899864196777\n",
            "Tensor Index: 448\n",
            "Similarity Score: 0.9999899864196777\n",
            "Tensor Index: 403\n",
            "Similarity Score: 0.9999896883964539\n",
            "Tensor Index: 396\n",
            "Similarity Score: 0.9999896883964539\n",
            "Tensor Index: 438\n",
            "Similarity Score: 0.9999752640724182\n",
            "Tensor Index: 453\n",
            "Similarity Score: 0.9999752640724182\n",
            "Tensor Index: 344\n",
            "Similarity Score: 0.9999671578407288\n",
            "Tensor Index: 359\n",
            "Similarity Score: 0.9999671578407288\n",
            "\n",
            "Top 10 most similar tensors (excluding self-similarity):\n",
            "Tensor Index: 273\n",
            "Similarity Score: 0.9999998807907104\n",
            "Tensor Index: 272\n",
            "Similarity Score: 0.9999998807907104\n",
            "Tensor Index: 390\n",
            "Similarity Score: 0.9999986290931702\n",
            "Tensor Index: 394\n",
            "Similarity Score: 0.9999986290931702\n",
            "Tensor Index: 363\n",
            "Similarity Score: 0.9999985694885254\n",
            "Tensor Index: 359\n",
            "Similarity Score: 0.9999985694885254\n",
            "Tensor Index: 372\n",
            "Similarity Score: 0.9999961256980896\n",
            "Tensor Index: 365\n",
            "Similarity Score: 0.9999961256980896\n",
            "Tensor Index: 431\n",
            "Similarity Score: 0.9999937415122986\n",
            "Tensor Index: 422\n",
            "Similarity Score: 0.9999937415122986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_tensor_beginner = torch.mean(torch.stack(most_similar_tensors_beginner), dim=0)\n",
        "mean_tensor_expert = torch.mean(torch.stack(most_similar_tensors_expert), dim=0)"
      ],
      "metadata": {
        "id": "4FTEP2uJIJP1"
      },
      "id": "4FTEP2uJIJP1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensors_list_beginner =  [t[1] for t in cls_embeddings_with_text if t[2].item() == 0]\n",
        "tensors_list_expert =  [t[1] for t in cls_embeddings_with_text if t[2].item() == 1]\n",
        "mean_tensor_beginner = torch.mean(torch.stack(tensors_list_beginner), dim=0)\n",
        "mean_tensor_expert = torch.mean(torch.stack(tensors_list_expert), dim=0)"
      ],
      "metadata": {
        "id": "UFChQU36fSFb"
      },
      "id": "UFChQU36fSFb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_tensor_beginner.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHVRcCojHqtk",
        "outputId": "0381d39e-548a-4a6c-dbb9-f304c335f8ce"
      },
      "id": "FHVRcCojHqtk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-14.5992, device='cuda:0', grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_tensor_expert.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gA8be6YIDtf",
        "outputId": "2a3dbb68-d003-4e1a-c2a5-a204334a9d11"
      },
      "id": "-gA8be6YIDtf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-14.2722, device='cuda:0', grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "correct = 0\n",
        "for t in cls_embeddings :\n",
        "  #print(t[2].item())\n",
        "  #print(t[1][0][:20])\n",
        "  print(\"truth : \")\n",
        "\n",
        "  if t[2].item() == 0 :\n",
        "    truth = \"beginner\"\n",
        "    print(\"beginner\")\n",
        "  else :\n",
        "    truth = \"expert\"\n",
        "    print(\"expert\")\n",
        "\n",
        "  cls_vector1_normalized = t[1][0].cpu().detach().numpy() / np.linalg.norm(t[1][0].cpu().detach().numpy())\n",
        "  cls_vector2_normalized = mean_tensor_beginner.cpu().detach().numpy() / np.linalg.norm(mean_tensor_beginner.cpu().detach().numpy())\n",
        "\n",
        "# Then, calculate cosine similarity\n",
        "  similarity_beg = cosine_similarity([cls_vector1_normalized], [cls_vector2_normalized])[0][0]\n",
        "\n",
        "  #print(\"Cosine Similarity beginner:\", similarity)\n",
        "\n",
        "# Assuming cls_vector1 and cls_vector2 are your CLS vectors\n",
        "# First, normalize the vectors\n",
        "  cls_vector1_normalized = t[1][0].cpu().detach().numpy() / np.linalg.norm(t[1][0].cpu().detach().numpy())\n",
        "  cls_vector2_normalized = mean_tensor_expert.cpu().detach().numpy() / np.linalg.norm(mean_tensor_expert.cpu().detach().numpy())\n",
        "\n",
        "# Then, calculate cosine similarity\n",
        "  similarity_expert = cosine_similarity([cls_vector1_normalized], [cls_vector2_normalized])[0][0]\n",
        "\n",
        "  #print(\"Cosine Similarity expert:\", similarity)\n",
        "\n",
        "  print(\"estimation : \")\n",
        "  if similarity_beg > similarity_expert and truth == \"beginner\" :\n",
        "    correct += 1\n",
        "    print(\"beginner\")\n",
        "  elif similarity_beg < similarity_expert and truth == \"expert\" :\n",
        "    correct += 1\n",
        "    print(\"expert\")\n",
        "\n",
        "print(\"correct : \", correct/len(cls_embeddings),\"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6Jz4ZAnLuXa",
        "outputId": "c7fda920-dd27-48c1-bf00-f63359f820f3"
      },
      "id": "c6Jz4ZAnLuXa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "truth : \n",
            "beginner\n",
            "estimation : \n",
            "truth : \n",
            "beginner\n",
            "estimation : \n",
            "truth : \n",
            "beginner\n",
            "estimation : \n",
            "beginner\n",
            "truth : \n",
            "expert\n",
            "estimation : \n",
            "expert\n",
            "truth : \n",
            "expert\n",
            "estimation : \n",
            "expert\n",
            "truth : \n",
            "expert\n",
            "estimation : \n",
            "expert\n",
            "truth : \n",
            "beginner\n",
            "estimation : \n",
            "truth : \n",
            "beginner\n",
            "estimation : \n",
            "beginner\n",
            "truth : \n",
            "beginner\n",
            "estimation : \n",
            "truth : \n",
            "expert\n",
            "estimation : \n",
            "expert\n",
            "truth : \n",
            "expert\n",
            "estimation : \n",
            "expert\n",
            "truth : \n",
            "expert\n",
            "estimation : \n",
            "expert\n",
            "truth : \n",
            "beginner\n",
            "estimation : \n",
            "beginner\n",
            "truth : \n",
            "beginner\n",
            "estimation : \n",
            "beginner\n",
            "truth : \n",
            "beginner\n",
            "estimation : \n",
            "beginner\n",
            "truth : \n",
            "expert\n",
            "estimation : \n",
            "expert\n",
            "truth : \n",
            "expert\n",
            "estimation : \n",
            "expert\n",
            "truth : \n",
            "expert\n",
            "estimation : \n",
            "expert\n",
            "correct :  0.7777777777777778 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO"
      ],
      "metadata": {
        "id": "M7JbrnRjGK1I"
      },
      "id": "M7JbrnRjGK1I"
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO : assemble results in clear diagram, try with more or less then 10 closest vectors, try with other embeddings, read prof notes\n",
        "# Get performance metrics and compare - have a baseline for comparaison\n",
        "# ensure code is right\n",
        "# ponderer avec le attention score mais utiliser un autre embedding tel que fast text?\n",
        "-------\n",
        "##summarize work, understand what i did\n",
        "#try with the embeddings of tf the other methods (ensure the tokenization and reconstruction is right)\n",
        "#try my final model\n",
        "#understand the numbers - why are the sums all almost equal?\n",
        "\n",
        "----\n",
        "#EXTRACT ATTENTION SCORES"
      ],
      "metadata": {
        "id": "PouD_n17_0Lo"
      },
      "id": "PouD_n17_0Lo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BROUILLON"
      ],
      "metadata": {
        "id": "amS9djRjGEk5"
      },
      "id": "amS9djRjGEk5"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "no_t_M-mz35W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "no_t_M-mz35W"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DaPgeCwwcUbl"
      },
      "id": "DaPgeCwwcUbl"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "\n",
        "class FullyConnectedNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(FullyConnectedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eeaOCjafz3X0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eeaOCjafz3X0"
    },
    {
      "cell_type": "code",
      "source": [
        "cls_embeddings_with_text[0][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFUWmkvrOv3Z",
        "outputId": "cbc045de-9940-4e37-f51d-0a480a3ee5b8"
      },
      "id": "sFUWmkvrOv3Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split([x[1].detach().cpu().numpy() for x in cls_embeddings_with_text], [x[2].detach().item() for x in cls_embeddings_with_text], test_size=0.2, random_state=42)\n",
        "\n",
        "#X_train, y_train = [x[1].detach().item() for x in cls_embeddings_with_text], [x[2].detach().item() for x in cls_embeddings_with_text]\n",
        "#X_val, y_val = [x[1].detach().item() for x in cls_embeddings_with_text], [x[2].detach().item() for x in cls_embeddings_with_text]\n",
        "#= vzs_test\n"
      ],
      "metadata": {
        "id": "1LzYvhubNxHJ"
      },
      "id": "1LzYvhubNxHJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NumPy arrays to PyTorch tensors\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "# Define the FullyConnectedNN model\n",
        "input_size = 768  # Size of the BERT [CLS] embeddings\n",
        "hidden_size = 128  # Example hidden size\n",
        "output_size = 2  # Example output size\n",
        "model = FullyConnectedNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "        val_preds = torch.argmax(val_outputs, dim=1)\n",
        "        val_accuracy = (val_preds == y_val_tensor).float().mean()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy.item():.4f}\")\n",
        "\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz6SO877N_ei",
        "outputId": "76e6fe36-4cb5-41bd-f689-cf4f98c16e2d"
      },
      "id": "vz6SO877N_ei",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6497, Val Loss: 0.2517, Val Accuracy: 1.0000\n",
            "Epoch 2/10, Loss: 0.2580, Val Loss: 0.0979, Val Accuracy: 1.0000\n",
            "Epoch 3/10, Loss: 0.1069, Val Loss: 0.0420, Val Accuracy: 1.0000\n",
            "Epoch 4/10, Loss: 0.0517, Val Loss: 0.0199, Val Accuracy: 1.0000\n",
            "Epoch 5/10, Loss: 0.0298, Val Loss: 0.0106, Val Accuracy: 1.0000\n",
            "Epoch 6/10, Loss: 0.0202, Val Loss: 0.0062, Val Accuracy: 1.0000\n",
            "Epoch 7/10, Loss: 0.0155, Val Loss: 0.0039, Val Accuracy: 1.0000\n",
            "Epoch 8/10, Loss: 0.0130, Val Loss: 0.0027, Val Accuracy: 1.0000\n",
            "Epoch 9/10, Loss: 0.0113, Val Loss: 0.0019, Val Accuracy: 1.0000\n",
            "Epoch 10/10, Loss: 0.0103, Val Loss: 0.0015, Val Accuracy: 1.0000\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-7eabae7df44b>:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NumPy arrays to PyTorch tensors\n",
        "def predict_final(vz) :\n",
        "  X_test_tensor = torch.tensor(vz, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Perform inference\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      test_outputs = model(X_test_tensor)\n",
        "      test_preds = torch.argmax(test_outputs)\n",
        "\n",
        "# Convert predictions tensor to NumPy array\n",
        "  test_preds_array = test_preds.numpy()\n",
        "\n",
        "# Now, test_preds_array contains the predicted class labels for the test data\n",
        "  print(\"Predictions:\", test_preds_array)\n",
        "  return test_preds_array"
      ],
      "metadata": {
        "id": "ml1WnbBmODUD"
      },
      "id": "ml1WnbBmODUD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for t in cls_embeddings :\n",
        "  print(t[2].item())\n",
        "  predict_final(t[1].detach().cpu().numpy())\n",
        "  if t[2].item() == t[1]:\n",
        "    count +=1\n",
        "\n",
        "  print(\"\\n\")\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_2K3Y7h9Rq1G",
        "outputId": "b97a776a-7c0a-4c5d-b18b-f3a18f3a676a"
      },
      "id": "_2K3Y7h9Rq1G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Predictions: 1\n",
            "tensor([[-4.6974e-01,  6.8680e-01, -1.7953e-01, -5.6552e-01,  5.9156e-01,\n",
            "          1.9117e-01,  5.9148e-01,  4.6054e-01,  2.8598e-01, -3.1942e-01,\n",
            "         -1.1019e+00,  1.1036e+00, -5.3062e-01,  1.1170e+00,  1.1479e+00,\n",
            "          2.0014e-01, -7.7582e-02,  8.1851e-01, -7.6001e-01, -4.4952e-01,\n",
            "         -1.1623e+00, -1.1406e-01,  1.5470e-01,  3.0982e-01,  1.6559e-01,\n",
            "         -1.1347e-01, -8.5904e-01,  6.9815e-02, -7.8200e-01,  2.0219e-01,\n",
            "          1.3745e-01, -5.3525e-01, -3.3343e-01, -1.2618e+00, -1.4071e-03,\n",
            "         -1.1734e+00,  3.1290e-01,  2.0768e-01, -3.2258e-01, -1.4172e+00,\n",
            "         -2.8726e-01,  8.4807e-01,  9.0239e-01, -1.6935e-01,  9.0878e-01,\n",
            "          6.9108e-01, -1.3807e+00, -4.3345e-03, -2.0109e-01,  4.0040e-01,\n",
            "         -1.1973e-01, -6.7484e-01,  4.4839e-01,  4.7229e-01, -3.0064e-01,\n",
            "          7.8256e-01, -4.3261e-01, -1.1440e+00, -3.5507e-01,  4.9777e-01,\n",
            "          8.6300e-01,  1.0937e+00,  3.9271e-01, -4.0738e-01,  5.2385e-01,\n",
            "         -3.0339e-01,  6.7470e-01, -5.8481e-02, -1.5388e+00, -4.2533e-01,\n",
            "         -2.0628e-01,  1.4636e-01,  5.9898e-01, -3.2939e-01, -3.7842e-01,\n",
            "          8.8505e-01,  2.7905e-01, -4.5378e-02, -1.2158e+00, -2.3138e-01,\n",
            "         -7.0593e-01,  5.7625e-01,  8.8663e-02, -6.1436e-01,  6.6849e-02,\n",
            "         -1.6285e-01,  1.2992e-01, -6.7123e-02,  5.8969e-01,  1.1043e-01,\n",
            "         -3.0683e-01, -7.2266e-01, -4.4741e-01, -6.1098e-01,  3.1715e-01,\n",
            "         -6.5506e-01, -1.0713e-01, -3.4422e-01, -9.5325e-01,  4.2100e-01,\n",
            "          1.0595e-01, -1.3795e-01, -4.9994e-03, -5.6270e-01,  7.8771e-01,\n",
            "          3.9084e-01, -7.3601e-02,  3.2765e-01, -7.0669e-01,  1.5858e-01,\n",
            "          7.7997e-01, -1.2451e+00,  9.6712e-01, -1.4800e+00,  1.3124e+00,\n",
            "          1.0556e-01, -1.5278e-01,  1.2109e+00, -1.6548e-01,  3.1227e-01,\n",
            "         -7.7296e-01,  1.6111e-01, -9.7425e-01, -5.1670e-02, -8.2203e-02,\n",
            "          2.3952e-01,  5.4419e-01, -7.5338e-01,  6.5702e-02,  3.3334e-01,\n",
            "          1.3215e+00, -7.7753e-01, -3.4222e-02,  3.9053e-01, -3.5374e-02,\n",
            "          7.8132e-01, -8.5863e-01,  1.7137e-01, -4.0378e-01,  9.6127e-01,\n",
            "          4.7814e-02, -4.3538e-01,  7.2868e-01, -7.2237e-02,  6.9514e-02,\n",
            "         -2.7752e-01,  1.2038e+00, -4.8272e-01, -8.2164e-01,  9.0959e-01,\n",
            "         -2.8290e-01, -1.2228e-01,  1.9556e-01, -2.0786e-01,  1.0466e+00,\n",
            "          5.1539e-01, -4.2020e-01,  1.4295e+00, -5.9881e-01,  5.8235e-02,\n",
            "         -1.8456e-01, -7.8678e-02, -7.2631e-01,  8.5427e-01, -3.7743e-01,\n",
            "          1.5076e+00,  1.8689e+00,  2.9097e-01, -1.8071e-01, -2.2376e-01,\n",
            "          2.6387e-02, -4.0573e-01,  7.9776e-01, -3.3578e-01, -9.4654e-01,\n",
            "          6.2202e-01,  6.8685e-01, -6.7292e-01,  8.6933e-02, -3.2316e-01,\n",
            "         -1.0508e+00,  5.3579e-01,  2.1223e-01, -5.4402e-01,  8.5606e-01,\n",
            "          2.3390e-01,  4.1830e-01, -5.3046e-01, -6.0133e-01,  2.0101e-01,\n",
            "          3.8527e-01, -5.9551e-01,  8.8215e-01, -6.0577e-01,  2.6945e-01,\n",
            "         -3.7414e-01,  1.0936e+00, -7.1485e-01,  2.9026e-01,  6.9611e-01,\n",
            "         -1.0700e+00, -2.8465e-01, -3.8231e-01,  2.8084e-01, -7.1137e-01,\n",
            "          2.3195e-01, -4.2204e-01,  6.2956e-01,  6.2323e-02, -8.6277e-01,\n",
            "          6.4708e-02,  1.0535e+00,  5.1673e-01, -9.2933e-01, -6.1045e-01,\n",
            "         -6.4217e-03,  6.7263e-01, -7.0793e-01, -2.1731e-01,  2.2025e-01,\n",
            "          8.7478e-01,  7.4425e-02, -9.1611e-01, -8.4482e-01, -7.8963e-02,\n",
            "          4.1889e-01, -3.4934e-01, -6.3648e-01,  3.1624e-01,  1.3017e-01,\n",
            "         -1.6397e+00, -1.2810e+00,  3.5559e-01,  3.3874e-01, -1.5111e-01,\n",
            "         -9.7954e-01, -4.8143e-02,  4.5113e-01, -3.9736e-01, -4.9122e-01,\n",
            "         -2.6496e-01, -4.0068e-01, -1.1930e-01,  1.2768e-01, -4.2394e-01,\n",
            "         -1.3867e-01, -5.1401e-01,  2.6368e-01,  4.8385e-01,  4.8777e-01,\n",
            "         -7.5571e-01,  7.3131e-02, -6.0415e-01, -4.7788e-02, -7.8532e-01,\n",
            "         -4.0683e-01, -4.1055e-02,  2.8832e-01, -6.5100e-01, -4.5594e-01,\n",
            "          4.4896e-01, -9.6556e-01,  5.4349e-01, -3.5662e-02, -6.3032e-01,\n",
            "          1.4377e-01,  1.2547e-01,  2.9803e-01, -5.2769e-01,  5.9488e-01,\n",
            "         -7.3490e-01, -2.4234e-01,  9.3148e-02,  7.3810e-01, -8.1023e-01,\n",
            "         -9.8603e-03,  4.0169e-01, -4.1667e-01, -3.8382e-01, -7.6250e-01,\n",
            "         -1.7630e-02,  1.0249e+00, -2.3648e-02,  5.9956e-01,  1.8301e-01,\n",
            "          4.7360e-01,  8.1183e-02, -9.2736e-01,  3.8393e-01,  8.6523e-01,\n",
            "         -8.3764e-01, -1.2636e+00,  3.8379e-01, -1.1394e+00, -1.1962e+00,\n",
            "         -1.0619e-01,  9.4214e-01, -4.9818e-01, -7.4583e-01,  4.3248e-01,\n",
            "         -3.1284e-01, -1.6074e+00,  9.0935e-02,  3.2019e-01,  4.4145e-01,\n",
            "          1.9139e-01,  5.8604e-01, -3.7074e-01,  2.2234e+00, -2.8910e-01,\n",
            "          7.0927e-01, -2.8615e-01,  1.5123e-01,  1.0304e+00, -4.0002e-01,\n",
            "         -5.2997e-01, -1.0426e+00, -8.1816e-01, -9.0900e-01, -1.0189e+00,\n",
            "          6.9141e-01,  7.5433e-01,  6.7389e-01, -6.0141e-01,  1.0147e-01,\n",
            "         -1.0240e+00, -1.0706e-01,  3.5512e-01, -3.9844e-01,  2.3308e-01,\n",
            "          2.3152e-03, -8.9547e-01,  7.5436e-01,  1.9025e-01, -1.7487e+00,\n",
            "          3.8613e-01, -3.6688e-01, -4.1477e-01,  4.7693e-01, -5.2538e-01,\n",
            "          5.2687e-01, -3.8493e-01, -5.2691e-01,  2.7827e-01, -1.2373e-02,\n",
            "          1.5629e-01,  1.1383e+00, -1.5186e+00, -5.2194e-02,  5.2414e-01,\n",
            "          4.7589e-01, -2.3750e-01,  5.6196e-01, -1.5633e-01,  5.6439e-01,\n",
            "          7.9003e-01,  3.3150e-01,  4.6315e-01, -7.6571e-01,  8.3614e-02,\n",
            "         -7.4340e-02,  2.8517e-01,  1.2350e-01,  3.9974e-01,  1.4608e-01,\n",
            "         -2.6868e-01, -2.0341e-01, -1.1576e+00,  5.5053e-01, -1.4978e+00,\n",
            "          4.0531e-01,  1.3151e+00, -3.3460e-01, -1.9963e-01, -7.6264e-01,\n",
            "         -8.0371e-02,  6.8724e-01,  1.0179e+00,  2.5328e-01,  9.2154e-01,\n",
            "         -2.8674e-01, -8.6434e-01, -1.5937e+00, -9.8841e-01, -4.7547e-02,\n",
            "         -1.0633e+00,  6.9306e-01, -5.9433e-01, -9.7620e-01, -4.9062e-01,\n",
            "         -4.0337e-01,  3.5787e-01,  1.4497e-01, -9.4535e-01,  7.8836e-01,\n",
            "          1.3096e-01, -1.2294e+00, -5.7288e-01, -1.3453e+00,  1.2176e+00,\n",
            "          8.3232e-01, -4.0963e-01,  2.9860e-01, -4.6382e-01, -4.3565e-01,\n",
            "         -1.1963e-01,  8.1013e-01,  7.1929e-01, -1.0900e-01, -5.3177e-01,\n",
            "         -1.0265e-01,  3.9204e-01,  4.3280e-02,  3.1350e-01,  4.0770e-01,\n",
            "         -2.4560e-01,  1.3957e-01,  5.5395e-01,  5.5348e-02, -1.2448e-01,\n",
            "          1.0927e-01, -1.1790e+00,  9.6459e-01,  7.2605e-02, -3.8770e-02,\n",
            "          1.0684e+00,  8.7908e-01, -3.1522e-01,  5.4561e-01, -1.1616e-01,\n",
            "          9.8917e-02,  1.2338e+00,  3.1325e-01,  7.1235e-01,  2.2206e-01,\n",
            "          2.5512e-01, -3.1273e-01, -1.3503e-01,  1.0107e-01,  8.3628e-01,\n",
            "         -5.5520e-01,  8.5901e-02,  8.5694e-02,  1.8242e-01,  9.7657e-01,\n",
            "         -1.4238e-01,  4.2253e-01,  3.4387e-02, -8.1968e-01, -3.6526e-01,\n",
            "         -4.0858e-01, -1.2679e-01,  4.4547e-01,  9.2411e-01,  1.0856e-01,\n",
            "         -2.0260e-01,  3.7465e-01, -6.6836e-01,  3.1239e-01,  8.7863e-01,\n",
            "          2.7433e-01, -5.0787e-01,  1.4493e+00, -1.0163e-01, -3.0684e-01,\n",
            "          3.6472e-01, -5.1029e-01, -6.9443e-01,  2.1790e-01,  8.5889e-01,\n",
            "         -6.1659e-01, -4.6277e-01, -2.6478e-01,  1.2841e+00, -1.9098e-01,\n",
            "         -4.8980e-01,  3.2294e-01,  6.1133e-01,  3.8609e-01, -3.6546e-01,\n",
            "          1.2011e-01, -1.0677e+00,  3.6146e-01,  4.5888e-01,  3.4447e-01,\n",
            "         -1.5575e-01, -2.4816e-01, -6.1427e-01, -1.3363e+00,  5.6586e-01,\n",
            "          3.8467e-01, -5.3166e-02, -7.9762e-01,  9.9578e-02, -7.7434e-01,\n",
            "         -8.5954e-01,  4.8491e-01,  1.2532e+00,  6.9607e-01,  4.8227e-01,\n",
            "         -8.3057e-01, -3.0782e-01, -1.1992e+00, -2.9055e-01,  6.4132e-01,\n",
            "         -2.3737e-01, -4.6165e-01, -2.2442e-02,  7.8295e-01,  1.1801e-01,\n",
            "         -5.2363e-01, -3.9332e-01,  2.2308e-01,  8.7597e-01,  6.0003e-01,\n",
            "         -4.9727e-01, -5.4110e-01,  2.2613e-01,  2.4287e-01, -2.2472e-03,\n",
            "          4.8221e-01, -8.3581e-01,  4.0739e-01, -5.4799e-01,  1.2198e+00,\n",
            "          6.0464e-02, -6.7911e-01,  4.8455e-01, -4.1964e-02, -9.2850e-01,\n",
            "         -4.0274e-01,  1.9188e-01, -2.8230e-02,  3.5894e-01, -2.6051e-02,\n",
            "         -1.0375e-01, -1.5272e+00, -8.5815e-01, -3.4444e-01, -1.0002e+00,\n",
            "         -5.5010e-01,  1.3966e-01, -5.2160e-01,  5.5096e-01,  1.1876e-01,\n",
            "         -2.1519e-02, -3.2284e-01, -1.2435e+00,  5.9756e-01,  3.6100e-01,\n",
            "          6.0881e-01, -5.5284e-01, -1.0030e+00, -2.6573e-01,  7.3790e-01,\n",
            "         -8.4465e-02, -2.6884e-01, -3.5020e-01, -2.9771e-02,  5.0588e-01,\n",
            "          4.3260e-02, -3.8025e-01, -1.7921e-01,  4.4288e-01,  4.7236e-01,\n",
            "         -6.6653e-01,  2.8458e-01, -1.1492e-01,  5.3043e-01, -2.5077e-01,\n",
            "         -3.9940e-01, -3.5040e-01,  1.8127e-01, -3.1191e-02,  3.1702e-01,\n",
            "         -1.4548e+00, -3.0908e-01, -8.6711e-01,  4.9462e-01,  8.2513e-01,\n",
            "          9.1501e-01, -2.6999e-01,  5.9502e-01,  1.5779e-01, -2.5700e-01,\n",
            "         -6.9223e-01, -3.8054e-01,  1.3547e-02,  2.2363e-01,  2.8458e-01,\n",
            "          8.3093e-02, -1.8042e-01, -3.5396e-01,  9.3102e-03,  8.1587e-03,\n",
            "          6.0302e-01, -2.9268e-01, -6.9910e-01, -3.7011e-01, -7.6245e-01,\n",
            "          4.6565e-02,  9.1627e-01,  2.3325e-01,  3.1643e-01, -2.5116e-01,\n",
            "         -4.4020e-01, -1.5758e-01, -7.8899e-01,  3.3583e-01,  8.6717e-01,\n",
            "         -6.5132e-01,  8.2359e-01,  3.6864e-01,  1.1221e-01,  2.7687e-01,\n",
            "         -2.6727e-01, -6.6922e-01,  7.1839e-01,  7.5184e-01,  6.3170e-01,\n",
            "          2.1173e-01, -1.5270e-02, -8.6437e-01, -6.7766e-01,  8.1797e-01,\n",
            "          8.5419e-01, -4.3694e-01, -6.1706e-01, -2.0337e-01, -9.9263e-02,\n",
            "         -9.0042e-02, -4.7388e-01,  6.3718e-02, -1.5924e-01,  7.7244e-01,\n",
            "         -2.6442e-01,  7.4651e-01,  1.1414e+00,  6.6010e-01,  4.9090e-01,\n",
            "          2.3948e-01,  6.1965e-01, -2.5352e-01,  1.0046e-05,  8.2863e-02,\n",
            "         -4.5424e-01, -7.4330e-01,  4.2822e-01, -9.1249e-01,  1.2401e+00,\n",
            "          2.3453e-02,  1.2325e-01,  3.5965e-01, -5.4044e-01, -3.4781e-01,\n",
            "          7.6549e-01, -5.2418e-01,  1.3683e-01,  7.5290e-01, -1.6795e-01,\n",
            "         -2.8464e-01,  5.2081e-01,  4.0495e-02,  5.2752e-01,  2.0703e-01,\n",
            "         -4.5944e-01, -4.2116e-01, -2.4388e-02,  2.8282e-01,  9.0024e-02,\n",
            "         -2.0057e-01, -9.3873e-01,  6.2201e-01,  2.5204e-01, -4.3216e-02,\n",
            "         -2.9270e-01, -6.0887e-01,  6.3386e-02,  3.9719e-01,  6.9043e-01,\n",
            "         -8.3376e-01, -3.0579e-01,  1.2924e-01,  4.6055e-01,  1.3965e-01,\n",
            "          3.3741e-01,  1.5758e-01, -1.5352e-02, -7.7649e-01,  8.3649e-02,\n",
            "         -5.2731e-01, -1.0960e+00,  5.0389e-01,  1.1109e-01, -7.0500e-01,\n",
            "          4.1680e-01,  9.5736e-01, -7.3511e-01,  1.1924e+00, -5.5783e-01,\n",
            "          7.4045e-01,  1.2593e-04,  8.9380e-02, -6.0133e-01,  2.3691e-01,\n",
            "         -1.9704e-01, -1.2601e-01, -6.8086e-01,  6.0034e-01,  4.7266e-01,\n",
            "         -8.7868e-01, -6.0648e-01, -5.0182e-01,  1.2583e-01,  5.5279e-01,\n",
            "         -3.7801e-02, -5.8142e-01, -6.4005e-01, -1.6756e-02,  9.1364e-01,\n",
            "         -1.4803e+00,  5.0910e-01, -3.0668e-01, -3.4067e-01,  4.3503e-01,\n",
            "         -9.4644e-02,  4.3682e-02, -8.7063e-01,  5.2207e-01,  4.0024e-01,\n",
            "          5.4118e-01,  4.8561e-01,  1.2501e-01, -2.5428e-01, -4.0019e-01,\n",
            "         -2.4466e-01, -5.1734e-01,  1.1070e+00, -8.5081e-01,  1.3662e-01,\n",
            "         -5.8135e-01,  5.6410e-02,  1.2840e+00, -2.2922e-01,  3.2109e-02,\n",
            "          2.0441e-01,  9.6122e-02, -1.5226e+00, -9.0835e-01,  1.0591e+00,\n",
            "         -8.8644e-01,  4.5853e-01,  7.5482e-01, -1.6313e-01,  1.4682e-01,\n",
            "         -1.3140e-01,  7.6656e-01,  2.9884e-01,  4.8969e-02, -4.7727e-01,\n",
            "          9.0359e-01, -9.3121e-01, -6.1722e-01, -7.3483e-01,  3.3506e-01,\n",
            "          7.1383e-02,  1.0303e-01,  4.7719e-01]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Boolean value of Tensor with more than one value is ambiguous",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-570bceabe62a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpredict_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}